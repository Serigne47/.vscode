# utils/vectorize_documents.py
"""
Script de vectorisation intelligent avec analyse LLM OPTIONNELLE
Utilise EnhancedChunker + IntelligentVectorStore
Mode par d√©faut : EMBEDDINGS ONLY pour √©conomiser les tokens
"""
import logging
import json
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
import os

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from utils.enhanced_chunker import EnhancedChunker
from utils.vectorstore import IntelligentVectorStore, create_intelligent_store

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vectorization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntelligentVectorizer:
    """
    Orchestrateur de vectorisation intelligent
    Combine chunking intelligent + analyse LLM OPTIONNELLE + indexation avanc√©e
    """
    
    def __init__(
        self,
        persist_directory: Path = Path("data/intelligent_store"),
        chunk_size: int = 2000,
        analyze_chunks: bool = False,  # CHANG√â : False par d√©faut pour √©conomiser
        use_cache: bool = True,
        embedding_only_mode: bool = True  # NOUVEAU : Mode embeddings only par d√©faut
    ):
        """
        Initialise le vectorizer intelligent
        
        Args:
            persist_directory: R√©pertoire de stockage
            chunk_size: Taille des chunks (2000 par d√©faut)
            analyze_chunks: Analyser avec LLM lors de l'indexation (D√âSACTIV√â par d√©faut)
            use_cache: Utiliser le cache pour √©viter re-traitement
            embedding_only_mode: Utiliser uniquement les embeddings (sans analyse LLM)
        """
        self.persist_directory = persist_directory
        self.chunk_size = chunk_size
        self.embedding_only_mode = embedding_only_mode
        
        # Si embedding only mode, forcer analyze_chunks √† False
        if embedding_only_mode:
            self.analyze_chunks = False
            logger.info("üí° Mode EMBEDDINGS ONLY activ√© - Analyse LLM d√©sactiv√©e")
        else:
            self.analyze_chunks = analyze_chunks
            
        self.use_cache = use_cache
        
        # Tracking
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'chunks_analyzed': 0,
            'concepts_extracted': 0,
            'time_elapsed': 0,
            'tokens_saved': 0  # NOUVEAU : Tracker les tokens √©conomis√©s
        }
        
        logger.info("üöÄ IntelligentVectorizer initialis√©")
        logger.info(f"   - Chunk size: {chunk_size}")
        logger.info(f"   - Analyse LLM: {'D√âSACTIV√âE (√©conomie)' if not self.analyze_chunks else 'Activ√©e'}")
        logger.info(f"   - Mode: {'EMBEDDINGS ONLY' if embedding_only_mode else 'COMPLET'}")
        logger.info(f"   - Cache: {use_cache}")
    
    def vectorize_from_folder(
        self,
        folder_path: Path,
        clear_existing: bool = False,
        save_report: bool = True
    ) -> IntelligentVectorStore:
        """
        Pipeline complet : Extraction ‚Üí Analyse ‚Üí Indexation
        
        Args:
            folder_path: Dossier contenant les documents
            clear_existing: R√©initialiser l'index existant
            save_report: Sauvegarder un rapport d√©taill√©
            
        Returns:
            IntelligentVectorStore configur√© et peupl√©
        """
        start_time = time.time()
        logger.info(f"üìÅ D√©marrage vectorisation depuis: {folder_path}")
        
        # ============================================================
        # PHASE 1: EXTRACTION INTELLIGENTE
        # ============================================================
        logger.info("\n" + "="*60)
        logger.info("PHASE 1: EXTRACTION INTELLIGENTE")
        logger.info("="*60)
        
        chunker = EnhancedChunker(
            chunk_size=self.chunk_size,
            chunk_overlap=200,
            intelligent_mode=True  # Mode intelligent activ√©
        )
        
        logger.info("üìÑ Extraction des documents en cours...")
        chunks = chunker.process_folder(folder_path, output_mode="intelligent")
        
        self.stats['documents_processed'] = len(set(
            c['metadata'].get('source', '') for c in chunks
        ))
        self.stats['chunks_created'] = len(chunks)
        
        logger.info(f"‚úÖ Extraction termin√©e:")
        logger.info(f"   - Documents trait√©s: {self.stats['documents_processed']}")
        logger.info(f"   - Chunks cr√©√©s: {self.stats['chunks_created']}")
        logger.info(f"   - Taille moyenne: {sum(c['metadata'].get('char_count', 0) for c in chunks) / len(chunks):.0f} chars")
        
        # Calculer les tokens √©conomis√©s
        if self.embedding_only_mode:
            tokens_per_chunk = 1500  # Estimation moyenne
            self.stats['tokens_saved'] = len(chunks) * tokens_per_chunk
            logger.info(f"   üí∞ Tokens √©conomis√©s: ~{self.stats['tokens_saved']:,} ({self.stats['tokens_saved'] * 0.00015:.2f}$)")
        
        # Sauvegarder les chunks extraits
        self._save_chunks(chunks)
        
        # ============================================================
        # PHASE 2: ENRICHISSEMENT ET PR√âPARATION
        # ============================================================
        logger.info("\n" + "="*60)
        logger.info("PHASE 2: ENRICHISSEMENT DES M√âTADONN√âES")
        logger.info("="*60)
        
        enriched_chunks = self._enrich_chunks(chunks)
        
        # ============================================================
        # PHASE 3: INDEXATION INTELLIGENTE
        # ============================================================
        logger.info("\n" + "="*60)
        logger.info("PHASE 3: INDEXATION " + ("EMBEDDINGS ONLY" if self.embedding_only_mode else "INTELLIGENTE"))
        logger.info("="*60)
        
        # Cr√©er ou r√©initialiser le store
        if clear_existing:
            logger.info("üóëÔ∏è Suppression de l'index existant...")
            store_path = self.persist_directory
            if store_path.exists():
                import shutil
                shutil.rmtree(store_path)
        
        # Cr√©er le vectorstore intelligent
        # IMPORTANT : Forcer analyze_on_index=False en mode embedding only
        vectorstore = create_intelligent_store(
            enriched_chunks,
            persist_directory=self.persist_directory,
            analyze_on_index=False if self.embedding_only_mode else self.analyze_chunks  # CHANG√â
        )
        
        # ============================================================
        # PHASE 4: VALIDATION ET RAPPORT
        # ============================================================
        logger.info("\n" + "="*60)
        logger.info("PHASE 4: VALIDATION ET RAPPORT")
        logger.info("="*60)
        
        self.stats['time_elapsed'] = time.time() - start_time
        
        # Tester quelques requ√™tes (SANS LLM en mode embedding only)
        validation_results = self._validate_vectorstore(vectorstore)
        
        # G√©n√©rer le rapport
        if save_report:
            report = self._generate_report(
                chunks, 
                vectorstore, 
                validation_results
            )
            self._save_report(report)
        
        logger.info(f"\n‚úÖ VECTORISATION COMPL√àTE EN {self.stats['time_elapsed']:.1f}s")
        logger.info(f"   - Index pr√™t: {vectorstore.get_stats()['total_documents']} documents")
        
        if self.embedding_only_mode:
            logger.info(f"   üí∞ MODE √âCONOMIQUE: Seuls les embeddings ont √©t√© cr√©√©s")
            logger.info(f"   üí° Pour activer l'analyse LLM: utilisez --enable-llm-analysis")
        else:
            logger.info(f"   - Concepts index√©s: {vectorstore.get_stats()['total_concepts']}")
        
        return vectorstore
    
    def vectorize_from_json(
        self,
        json_file: Path,
        clear_existing: bool = False
    ) -> IntelligentVectorStore:
        """
        Vectorise depuis un fichier JSON de chunks d√©j√† extraits
        
        Args:
            json_file: Fichier JSON contenant les chunks
            clear_existing: R√©initialiser l'index
            
        Returns:
            IntelligentVectorStore configur√©
        """
        logger.info(f"üìÑ Chargement depuis JSON: {json_file}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        logger.info(f"‚úÖ {len(chunks)} chunks charg√©s")
        
        # Calculer les tokens √©conomis√©s
        if self.embedding_only_mode:
            tokens_per_chunk = 1500
            self.stats['tokens_saved'] = len(chunks) * tokens_per_chunk
            logger.info(f"üí∞ Mode √©conomique: ~{self.stats['tokens_saved']:,} tokens √©conomis√©s")
        
        # Enrichir si n√©cessaire
        enriched_chunks = self._enrich_chunks(chunks)
        
        # Cr√©er le store
        if clear_existing:
            store_path = self.persist_directory
            if store_path.exists():
                import shutil
                shutil.rmtree(store_path)
        
        # IMPORTANT : Forcer analyze_on_index=False en mode embedding only
        vectorstore = create_intelligent_store(
            enriched_chunks,
            persist_directory=self.persist_directory,
            analyze_on_index=False if self.embedding_only_mode else self.analyze_chunks  # CHANG√â
        )
        
        return vectorstore
    
    def _enrich_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        Enrichit les chunks avec m√©tadonn√©es suppl√©mentaires
        """
        logger.info("üîß Enrichissement des m√©tadonn√©es...")
        
        enriched = []
        doc_structure = {}
        
        for i, chunk in enumerate(chunks):
            # Tracker la structure du document
            doc_name = chunk['metadata'].get('source', 'unknown')
            if doc_name not in doc_structure:
                doc_structure[doc_name] = {
                    'sections': set(),
                    'pages': set(),
                    'chunk_count': 0
                }
            
            # Enrichir les m√©tadonn√©es
            chunk['metadata']['chunk_index'] = i
            chunk['metadata']['total_chunks'] = len(chunks)
            
            # Ajouter position relative
            chunk['metadata']['position_relative'] = i / len(chunks)
            if i < len(chunks) * 0.2:
                chunk['metadata']['document_zone'] = 'introduction'
            elif i < len(chunks) * 0.8:
                chunk['metadata']['document_zone'] = 'body'
            else:
                chunk['metadata']['document_zone'] = 'conclusion'
            
            # Tracker structure
            if section := chunk['metadata'].get('section'):
                doc_structure[doc_name]['sections'].add(section)
            if page := chunk['metadata'].get('page'):
                doc_structure[doc_name]['pages'].add(page)
            doc_structure[doc_name]['chunk_count'] += 1
            
            enriched.append(chunk)
        
        # Afficher structure d√©couverte
        logger.info("üìä Structure des documents:")
        for doc_name, structure in doc_structure.items():
            logger.info(f"   {doc_name}:")
            logger.info(f"     - Sections: {len(structure['sections'])}")
            logger.info(f"     - Pages: {len(structure['pages'])}")
            logger.info(f"     - Chunks: {structure['chunk_count']}")
        
        return enriched
    
    def _validate_vectorstore(
        self, 
        vectorstore: IntelligentVectorStore
    ) -> Dict[str, Any]:
        """
        Valide le vectorstore avec des requ√™tes de test
        """
        logger.info("üß™ Validation du vectorstore...")
        
        test_queries = [
            "What is the submission deadline?",
            "What are the payment terms and conditions?",
            "Technical requirements for the system",
            "Insurance and liability clauses",
            "Transport volumes and logistics"
        ]
        
        results = {}
        for query in test_queries:
            try:
                search_results = vectorstore.intelligent_search(
                    query, 
                    k=3,
                    refine_with_llm=False  # TOUJOURS False pour validation rapide
                )
                results[query] = {
                    'found': len(search_results),
                    'top_score': search_results[0]['score'] if search_results else 0
                }
                logger.info(f"   ‚úì '{query[:30]}...': {len(search_results)} r√©sultats")
            except Exception as e:
                results[query] = {'found': 0, 'error': str(e)}
                logger.warning(f"   ‚úó '{query[:30]}...': Erreur - {e}")
        
        return results
    
    def _save_chunks(self, chunks: List[Dict]):
        """
        Sauvegarde les chunks extraits
        """
        output_file = self.persist_directory / "extracted_chunks.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üíæ Chunks sauvegard√©s: {output_file}")
    
    def _generate_report(
        self,
        chunks: List[Dict],
        vectorstore: IntelligentVectorStore,
        validation_results: Dict
    ) -> Dict[str, Any]:
        """
        G√©n√®re un rapport d√©taill√© de vectorisation
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'mode': 'EMBEDDINGS_ONLY' if self.embedding_only_mode else 'FULL_ANALYSIS',
            'configuration': {
                'chunk_size': self.chunk_size,
                'analyze_chunks': self.analyze_chunks,
                'embedding_only_mode': self.embedding_only_mode,
                'persist_directory': str(self.persist_directory)
            },
            'statistics': self.stats,
            'chunks_analysis': {
                'total': len(chunks),
                'by_type': {},
                'by_document': {},
                'average_size': sum(c['metadata'].get('char_count', 0) for c in chunks) / len(chunks)
            },
            'vectorstore_stats': vectorstore.get_stats(),
            'validation': validation_results,
            'cost_estimate': {
                'embeddings_cost': len(chunks) * 0.00002,  # Estimation
                'llm_analysis_cost': 0 if self.embedding_only_mode else len(chunks) * 0.03,
                'total_cost': len(chunks) * (0.00002 if self.embedding_only_mode else 0.03002)
            }
        }
        
        # Analyser distribution des chunks
        for chunk in chunks:
            chunk_type = chunk.get('type', 'unknown')
            doc_name = chunk['metadata'].get('source', 'unknown')
            
            report['chunks_analysis']['by_type'][chunk_type] = \
                report['chunks_analysis']['by_type'].get(chunk_type, 0) + 1
            
            report['chunks_analysis']['by_document'][doc_name] = \
                report['chunks_analysis']['by_document'].get(doc_name, 0) + 1
        
        return report
    
    def _save_report(self, report: Dict):
        """
        Sauvegarde le rapport de vectorisation
        """
        report_file = self.persist_directory / f"vectorization_report_{datetime.now():%Y%m%d_%H%M%S}.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"üìä Rapport sauvegard√©: {report_file}")


def quick_test(vectorstore: IntelligentVectorStore):
    """
    Test rapide du vectorstore (SANS analyse LLM co√ªteuse)
    """
    print("\n" + "="*60)
    print("TEST RAPIDE DU VECTORSTORE (MODE √âCONOMIQUE)")
    print("="*60)
    
    # Requ√™tes de test
    test_cases = [
        {
            'query': "deadline for submission",
            'expected_concepts': ['deadline', 'timeline', 'submission']
        },
        {
            'query': "payment terms and pricing",
            'expected_concepts': ['payment', 'pricing', 'financial']
        },
        {
            'query': "technical specifications",
            'expected_concepts': ['technical', 'requirements', 'specifications']
        }
    ]
    
    for test in test_cases:
        print(f"\nüîç Test: {test['query']}")
        print("-" * 40)
        
        results = vectorstore.intelligent_search(
            test['query'],
            k=2,
            refine_with_llm=False  # CHANG√â : False pour √©conomiser
        )
        
        for i, result in enumerate(results, 1):
            print(f"\nüìÑ R√©sultat {i}:")
            print(f"   Score: {result.get('score', 0):.2f}")
            if 'source' in result:
                print(f"   Source: {result['source'].get('breadcrumb', 'N/A')}")
            print(f"   Confidence: {result.get('confidence', 0)}%")
            print(f"   Extrait: {result.get('text', '')[:150]}...")


def main():
    """
    Point d'entr√©e principal
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Vectorisation Intelligente (Mode √âconomique par d√©faut)"
    )
    parser.add_argument(
        "--source", 
        type=str, 
        required=True,
        help="Dossier de documents ou fichier JSON de chunks"
    )
    parser.add_argument(
        "--output", 
        type=str, 
        default="data/intelligent_store",
        help="R√©pertoire de sortie pour l'index"
    )
    parser.add_argument(
        "--chunk-size", 
        type=int, 
        default=2000,
        help="Taille des chunks (d√©faut: 2000)"
    )
    parser.add_argument(
        "--enable-llm-analysis",  # CHANG√â : Nouveau flag pour ACTIVER l'analyse
        action="store_true",
        help="Activer l'analyse LLM (co√ªteux - d√©sactiv√© par d√©faut)"
    )
    parser.add_argument(
        "--no-analysis",  # Gard√© pour compatibilit√©
        action="store_true",
        help="[DEPRECATED] Utilisez --enable-llm-analysis pour activer l'analyse"
    )
    parser.add_argument(
        "--clear", 
        action="store_true",
        help="Supprimer l'index existant"
    )
    parser.add_argument(
        "--test", 
        action="store_true",
        help="Lancer les tests apr√®s vectorisation"
    )
    
    args = parser.parse_args()
    
    # V√©rifier OpenAI API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
        print("   D√©finissez-la avec: export OPENAI_API_KEY='votre-cl√©'")
        return
    
    source_path = Path(args.source)
    
    if not source_path.exists():
        print(f"‚ùå Chemin introuvable: {source_path}")
        return
    
    # D√©terminer le mode
    embedding_only = not args.enable_llm_analysis  # Par d√©faut: embedding only
    
    if embedding_only:
        print("\nüí∞ MODE √âCONOMIQUE ACTIV√â")
        print("   - Utilisation des embeddings uniquement")
        print("   - Pas d'analyse LLM (√©conomie ~99% des tokens)")
        print("   - Pour activer l'analyse LLM: ajoutez --enable-llm-analysis")
    else:
        print("\n‚ö†Ô∏è MODE ANALYSE COMPL√àTE ACTIV√â")
        print("   - Analyse LLM pour chaque chunk")
        print("   - Co√ªt estim√©: ~$0.03 par chunk")
        response = input("   Continuer? (y/n): ")
        if response.lower() != 'y':
            print("Annul√©.")
            return
    
    try:
        # Cr√©er le vectorizer
        vectorizer = IntelligentVectorizer(
            persist_directory=Path(args.output),
            chunk_size=args.chunk_size,
            analyze_chunks=not embedding_only,  # Analyse seulement si explicitement demand√©
            embedding_only_mode=embedding_only  # Mode embedding only par d√©faut
        )
        
        # Lancer la vectorisation
        if source_path.suffix == '.json':
            vectorstore = vectorizer.vectorize_from_json(
                json_file=source_path,
                clear_existing=args.clear
            )
        elif source_path.is_dir():
            vectorstore = vectorizer.vectorize_from_folder(
                folder_path=source_path,
                clear_existing=args.clear
            )
        else:
            raise ValueError(f"Source invalide (doit √™tre dossier ou .json): {source_path}")
        
        # Tests si demand√©s
        if args.test:
            quick_test(vectorstore)
        
        print("\n‚úÖ VECTORISATION TERMIN√âE AVEC SUCC√àS!")
        print(f"   Index sauvegard√© dans: {args.output}")
        
        if embedding_only:
            print(f"\nüí∞ √âCONOMIES R√âALIS√âES:")
            print(f"   - Tokens √©conomis√©s: ~{vectorizer.stats.get('tokens_saved', 0):,}")
            print(f"   - Co√ªt √©vit√©: ~${vectorizer.stats.get('tokens_saved', 0) * 0.00002:.2f}")
            print(f"   - Temps gagn√©: ~{vectorizer.stats.get('tokens_saved', 0) // 100} secondes")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())