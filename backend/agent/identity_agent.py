# agents/identity_agent.py
"""
Agent d'identit√© simplifi√© pour GPT-4o-mini
Version am√©lior√©e avec tra√ßabilit√© compl√®te des chunks
"""

import logging
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict, field
import yaml

# 250822 Nouveau : 
import pickle  # AJOUTER
import hashlib  # AJOUTER
from typing import Dict, List, Any, Optional, Tuple  # Ajouter Tuple si pas d√©j√† pr√©sent

import sys
sys.path.append(str(Path(__file__).parent.parent))

from openai import OpenAI
from langchain.prompts import PromptTemplate

# Import des composants
from utils.vectorstore import IntelligentVectorStore
from utils.enhanced_retrieval import IntelligentRetriever, RetrievalResult

logger = logging.getLogger(__name__)

@dataclass
class IdentityQuestion:
    """Structure d'une question d'identit√©"""
    id: str
    system_prompt: str
    validator: Dict[str, Any] = None
    
@dataclass
class IdentityAnswer:
    """
    R√©ponse structur√©e avec tra√ßabilit√© compl√®te
    Inclut maintenant les chunks complets utilis√©s pour l'extraction
    """
    question_id: str
    answer: Any
    sources: List[Dict[str, str]]
    confidence: float
    status: str  # success, partial, failed
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)  # NOUVEAU: chunks complets avec m√©tadonn√©es
    
    def to_dict(self) -> Dict:
        """Convertit en dictionnaire avec tous les champs"""
        return asdict(self)


class SimpleIdentityAgent:
    """
    Agent d'extraction d'identit√© simplifi√© pour GPT-4o-mini
    Version am√©lior√©e avec tra√ßabilit√© compl√®te des sources
    """
    
    def __init__(
        self,
        vectorstore: IntelligentVectorStore,
        retriever: IntelligentRetriever,
        config_path: str = "configs/prompts/identity/en.yaml"
    ):
        """
        Initialise l'agent simplifi√©
        
        Args:
            vectorstore: IntelligentVectorStore configur√©
            retriever: IntelligentRetriever configur√©
            config_path: Chemin vers le YAML des questions
        """
        self.vectorstore = vectorstore
        self.retriever = retriever
        
        # Client OpenAI direct pour GPT-5-mini
        self.client = OpenAI()
        self.model = "gpt-5-mini"  # ou "gpt-5" pour plus de puissance
        
        # Charger les questions
        self.questions = self._load_questions(config_path)

        # NOUVEAU : Initialiser le syst√®me de cache
        self.cache_dir = Path("data/cache/identity_agent")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
        # NOUVEAU : Queries optimis√©es pour chaque question
        self.optimized_queries = {
            "identity.client_name": "client company name official legal entity issuing tender tendering party",
            "identity.tender_reference": "tender reference number RFP RFQ code identifier official",
            "identity.timeline_milestones": "deadline submission date timeline milestones key dates schedule",
            "identity.submission_channel": "submit submission email portal platform how where send",
            "identity.expected_deliverables": "deliverables documents required expected submit provide",
            "identity.operating_countries": "countries regions geographical scope territories location operation",
            "identity.service_main_scope": "service scope main primary transport logistics activities",
            "identity.contract_type": "contract type framework agreement nature legal structure",
            "identity.contract_duration": "contract duration period years months term length validity"
        }
    
        # NOUVEAU : Pr√©-charger ou calculer les embeddings
        self.embeddings_cache = self._initialize_embeddings_cache()
        
        # Stats basiques
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'total_chunks_used': 0,  # NOUVEAU: tracking des chunks utilis√©s
            'cache_hits': 0,  # NOUVEAU
            'cache_misses': 0  # NOUVEAU
        }
        
        logger.info(f"‚úÖ Agent initialis√© avec {len(self.questions)} questions")

    def _initialize_embeddings_cache(self) -> Dict[str, Any]:
        """
        Initialise le cache d'embeddings persistant
        Charge depuis le disque ou calcule si n√©cessaire
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        
        # Essayer de charger le cache existant
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cache = pickle.load(f)
                    logger.info(f"‚úÖ Cache charg√© depuis {cache_file}")
                    
                    # V√©rifier que toutes les questions sont dans le cache
                    missing = set(self.optimized_queries.keys()) - set(cache.keys())
                    if missing:
                        logger.info(f"‚ö†Ô∏è Questions manquantes dans le cache: {missing}")
                        # Compl√©ter le cache avec les questions manquantes
                        for question_id in missing:
                            cache = self._add_to_cache(cache, question_id)
                        # Sauvegarder le cache mis √† jour
                        self._save_cache(cache)
                    
                    return cache
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de charger le cache: {e}")
        
        # Si pas de cache, le cr√©er
        logger.info("üîÑ Cr√©ation du cache d'embeddings...")
        cache = {}
        
        for question_id, query in self.optimized_queries.items():
            cache = self._add_to_cache(cache, question_id)
        
        # Sauvegarder le cache
        self._save_cache(cache)
        logger.info(f"‚úÖ Cache cr√©√© avec {len(cache)} embeddings")
        
        return cache

    def _add_to_cache(self, cache: Dict, question_id: str) -> Dict:
        """
        Ajoute un embedding au cache
        """
        query = self.optimized_queries.get(question_id)
        if query:
            logger.info(f"  üìù Calcul embedding pour: {question_id}")
            embedding = self.vectorstore.embeddings.embed_query(query)
            cache[question_id] = {
                'query': query,
                'embedding': embedding,
                'created_at': datetime.now().isoformat()
            }
        return cache

    def _save_cache(self, cache: Dict):
        """
        Sauvegarde le cache sur disque
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(cache, f)
        logger.info(f"üíæ Cache sauvegard√©: {cache_file}")
    
    def _load_questions(self, config_path: str) -> List[IdentityQuestion]:
        """Charge les questions depuis le YAML"""
        questions = []
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            for item in config:
                question = IdentityQuestion(
                    id=item['id'],
                    system_prompt=item['system_prompt'],
                    validator=item.get('validator', {})
                )
                questions.append(question)
            
            logger.info(f"üìã {len(questions)} questions charg√©es")
            
        except Exception as e:
            logger.error(f"Erreur chargement config: {e}")
            raise
        
        return questions
    
    def extract_all(self) -> Dict[str, Any]:
        """
        Extrait toutes les informations d'identit√© avec tra√ßabilit√© compl√®te
        
        Returns:
            Dictionnaire avec toutes les r√©ponses et leurs sources d√©taill√©es
        """
        logger.info("\n" + "="*60)
        logger.info("üîç EXTRACTION D'IDENTIT√â AVEC TRA√áABILIT√â")
        logger.info("="*60)
        
        start_time = datetime.now()
        
        results = {
            'timestamp': start_time.isoformat(),
            'questions': {},
            'summary': {},
            'metadata': {  # NOUVEAU: m√©tadonn√©es globales
                'total_chunks_analyzed': 0,
                'documents_referenced': set()
            }
        }
        
        # Traiter chaque question
        for question in self.questions:
            answer = self.extract_single(question)
            results['questions'][answer.question_id] = answer.to_dict()
            
            # Collecter les m√©tadonn√©es globales
            for chunk in answer.evidence_chunks:
                if 'metadata' in chunk and 'source' in chunk['metadata']:
                    results['metadata']['documents_referenced'].add(
                        chunk['metadata']['source']
                    )
            results['metadata']['total_chunks_analyzed'] += len(answer.evidence_chunks)
        
        # Convertir le set en liste pour la s√©rialisation JSON
        results['metadata']['documents_referenced'] = list(
            results['metadata']['documents_referenced']
        )
        
        # G√©n√©rer le r√©sum√©
        results['summary'] = self._generate_summary(results['questions'])
        
        # Stats finales
        elapsed = (datetime.now() - start_time).total_seconds()
        results['stats'] = {
            **self.stats,
            'time_seconds': elapsed,
            'avg_chunks_per_question': (
                self.stats['total_chunks_used'] / self.stats['total'] 
                if self.stats['total'] > 0 else 0
            )
        }
        
        logger.info(f"\n‚úÖ Extraction termin√©e en {elapsed:.1f}s")
        logger.info(f"   Succ√®s: {self.stats['success']}/{self.stats['total']}")
        logger.info(f"   Chunks utilis√©s: {self.stats['total_chunks_used']}")
        
        return results
    
    def extract_single(self, question: IdentityQuestion) -> IdentityAnswer:
        """
        Extrait l'information pour une question avec cache optimis√©
        
        Args:
            question: Question √† traiter
            
        Returns:
            IdentityAnswer avec la r√©ponse et tous les chunks utilis√©s
        """
        self.stats['total'] += 1
        logger.info(f"\nüîç Traitement: {question.id}")
        
        try:
            # NOUVEAU : Utiliser le cache si disponible
            if question.id in self.embeddings_cache:
                # Recherche optimis√©e avec embedding cach√©
                retrieval_result = self._retrieve_with_cached_embedding(
                    question_id=question.id,
                    category="identity"
                )
                self.stats['cache_hits'] += 1
                logger.debug(f"   ‚úÖ Cache hit pour {question.id}")
            else:
                # Fallback : m√©thode normale (pour questions custom)
                search_query = self._create_search_query(question)
                retrieval_result = self.retriever.retrieve_and_answer(
                    query=search_query,
                    category="identity",
                    require_source=True,
                    max_chunks=3
                )
                self.stats['cache_misses'] += 1
                logger.debug(f"   ‚ö†Ô∏è Cache miss pour {question.id}")
            
            # 3. Extraire la r√©ponse structur√©e avec les chunks complets
            answer = self._extract_answer_with_evidence(question, retrieval_result)
            
            # Mise √† jour des stats
            if answer.status == "success":
                self.stats['success'] += 1
            else:
                self.stats['failed'] += 1
            
            self.stats['total_chunks_used'] += len(answer.evidence_chunks)
            
            logger.info(f"   ‚úÖ Status: {answer.status} (confiance: {answer.confidence:.0%})")
            logger.info(f"   üìÑ Chunks utilis√©s: {len(answer.evidence_chunks)}")
            
            return answer
            
        except Exception as e:
            logger.error(f"   ‚ùå Erreur: {e}")
            self.stats['failed'] += 1
            
            return IdentityAnswer(
                question_id=question.id,
                answer=None,
                sources=[],
                confidence=0.0,
                status="failed",
                evidence_chunks=[]
            )
    def _retrieve_with_cached_embedding(
        self,
        question_id: str,
        category: str
    ) -> RetrievalResult:
        """
        Effectue une recherche en utilisant l'embedding pr√©-calcul√©
        √âvite l'appel API pour embed_query
        """
        cached_data = self.embeddings_cache[question_id]
        query = cached_data['query']
        embedding = cached_data['embedding']
        
        # Recherche directe dans le vectorstore avec l'embedding
        # Au lieu d'appeler intelligent_search qui recalculerait l'embedding
        results = self._search_vectorstore_with_embedding(
            embedding=embedding,
            k=3
        )
        
        # Construire un RetrievalResult compatible
        chunks = []
        sources = []
        
        for result in results:
            chunks.append(result)
            if 'source' in result:
                sources.append(result['source'])
        
        # Cr√©er une r√©ponse simple bas√©e sur les chunks
        answer_text = f"Information found in {len(chunks)} chunks"
        
        return RetrievalResult(
            query=query,
            answer=answer_text,
            chunks=chunks,
            sources=sources[:3],
            confidence=0.8 if chunks else 0.0,
            metadata={'from_cache': True}
        )

    def _search_vectorstore_with_embedding(
        self,
        embedding: List[float],
        k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        Recherche directe dans FAISS avec un embedding pr√©-calcul√©
        """
        import numpy as np
        
        # Convertir en numpy array
        query_vector = np.array([embedding], dtype=np.float32)
        
        # Normaliser pour FAISS (si n√©cessaire)
        if hasattr(self.vectorstore, 'index') and self.vectorstore.index:
            import faiss
            faiss.normalize_L2(query_vector)
            
            # Recherche dans FAISS
            distances, indices = self.vectorstore.index.search(query_vector, k)
            
            # R√©cup√©rer les chunks correspondants
            results = []
            for idx, dist in zip(indices[0], distances[0]):
                if idx >= 0 and idx in self.vectorstore.id_to_text:
                    chunk_result = {
                        'text': self.vectorstore.id_to_text[idx],
                        'metadata': self.vectorstore.id_to_metadata.get(idx, {}),
                        'score': float(dist),
                        'source': self.vectorstore.id_to_metadata.get(idx, {})
                    }
                    results.append(chunk_result)
            
            return results
        else:
            # Fallback si pas d'acc√®s direct √† FAISS
            logger.warning("Pas d'acc√®s direct √† FAISS, utilisation de la recherche normale")
            return []
    
    def _create_search_query(self, question: IdentityQuestion) -> str:
        """
        Retourne la query optimis√©e depuis le mapping
        """
        # Utiliser les queries optimis√©es d√©finies dans __init__
        return self.optimized_queries.get(
            question.id, 
            "tender information details"  # Fallback g√©n√©rique
        )
    
    def _extract_answer_with_evidence(
        self,
        question: IdentityQuestion,
        retrieval_result: RetrievalResult
    ) -> IdentityAnswer:
        """
        Extrait la r√©ponse depuis les chunks trouv√©s avec tra√ßabilit√© compl√®te
        
        Args:
            question: La question √† traiter
            retrieval_result: R√©sultats du retrieval avec chunks
            
        Returns:
            IdentityAnswer enrichi avec evidence_chunks
        """
        # Pr√©parer le contexte ET capturer les chunks info
        context, chunks_details = self._prepare_context_with_tracking(
            retrieval_result.chunks[:3]
        )
        
        # Construire evidence_chunks avec toutes les m√©tadonn√©es
        evidence_chunks = self._build_evidence_chunks(
            retrieval_result.chunks[:3],
            chunks_details
        )
        
        # Prompt simplifi√© pour extraction JSON
        prompt = f"""{question.system_prompt}

Context from documents:
{context}

IMPORTANT: Return ONLY a valid JSON object. No explanations.
The JSON must start with {{ and end with }}.
"""
        
        try:
            # Appeler le LLM GPT-5-mini avec le contexte COMPLET
            response = self.client.responses.create(
                model=self.model,
                input=f"{question.system_prompt}\n\nContext from documents:\n{context}",
                reasoning={"effort": "minimal"},  # Pour extraction rapide
                text={"verbosity": "low"}  # R√©ponses concises JSON
            )
            result_text = response.output_text.strip()
            
            # Extraire le JSON de la r√©ponse
            json_obj = self._extract_json(result_text)
            
            # Cr√©er la r√©ponse enrichie
            return IdentityAnswer(
                question_id=question.id,
                answer=json_obj.get('answer'),
                sources=[],
                confidence=retrieval_result.confidence,
                status="success" if json_obj.get('answer') else "partial",
                evidence_chunks=evidence_chunks  # NOUVEAU: chunks complets avec m√©tadonn√©es
            )
            
        except Exception as e:
            logger.debug(f"Erreur extraction: {e}")
            
            # Fallback: utiliser la r√©ponse directe du retrieval
            return IdentityAnswer(
                question_id=question.id,
                answer=retrieval_result.answer if retrieval_result.answer else None,
                sources=[],
                confidence=retrieval_result.confidence * 0.5,
                status="partial",
                evidence_chunks=evidence_chunks  # Inclure m√™me en cas de fallback
            )
    
    def _prepare_context_with_tracking(
        self, 
        chunks: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        """
        Pr√©pare le contexte depuis les chunks avec TEXTE COMPLET
        et retourne aussi les d√©tails pour tra√ßabilit√©
        
        Args:
            chunks: Liste des chunks du retrieval
            
        Returns:
            Tuple (context_string, chunks_details)
        """
        context_parts = []
        chunks_details = []
        
        for i, chunk in enumerate(chunks):
            # Extraire les m√©tadonn√©es
            source = chunk.get('source', {})
            metadata = chunk.get('metadata', {})
            
            # Utiliser le TEXTE COMPLET du chunk (pas de limitation!)
            text_full = chunk.get('text', '')
            
            # Info source pour le contexte
            source_info = f"[Doc: {source.get('document', 'Unknown')} | Section: {source.get('section', 'N/A')}]"
            
            # Ajouter au contexte avec le texte COMPLET
            context_parts.append(f"{source_info}\n{text_full}")
            
            # Sauvegarder les d√©tails pour evidence_chunks
            chunks_details.append({
                'index': i,
                'text': text_full,  # Texte COMPLET
                'source': source.get('document', 'Unknown'),
                'section': source.get('section', 'N/A'),
                'metadata': metadata  # Toutes les m√©tadonn√©es disponibles
            })
        
        context_string = "\n---\n".join(context_parts)
        
        logger.debug(f"   Context pr√©par√©: {len(context_string)} caract√®res depuis {len(chunks)} chunks")
        
        return context_string, chunks_details
    
    def _build_evidence_chunks(
        self,
        original_chunks: List[Dict],
        chunks_details: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        Construit la structure evidence_chunks avec toutes les m√©tadonn√©es
        
        Args:
            original_chunks: Chunks originaux du retrieval
            chunks_details: D√©tails extraits par _prepare_context_with_tracking
            
        Returns:
            Liste structur√©e pour evidence_chunks
        """
        evidence_chunks = []
        
        for chunk, details in zip(original_chunks, chunks_details):
            evidence_chunk = {
                "chunk_index": details['index'],
                "text_full": details['text'],  # Texte COMPLET du chunk
                "metadata": {
                    "source": details['source'],
                    "section": details['section']
                },
                "relevance_score": chunk.get('score', 0.0) if 'score' in chunk else chunk.get('confidence', 0.0)
            }
            
            
            evidence_chunks.append(evidence_chunk)
        
        return evidence_chunks
    
    def _extract_json(self, text: str) -> Dict:
        """
        Extrait un objet JSON depuis du texte
        G√®re les cas o√π le LLM ajoute du texte avant/apr√®s
        """
        # Nettoyer le texte
        text = text.strip()
        
        # Chercher le JSON
        if '{' in text and '}' in text:
            start = text.find('{')
            end = text.rfind('}') + 1
            json_str = text[start:end]
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.debug(f"Premi√®re tentative JSON √©chou√©e: {e}")
                # Essayer de nettoyer et r√©essayer
                json_str = json_str.replace('\n', ' ').replace('\r', '')
                # Retirer les commentaires potentiels
                json_str = ' '.join(line.split('//')[0] for line in json_str.split('\n'))
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as e2:
                    logger.error(f"Impossible d'extraire JSON: {e2}")
                    logger.debug(f"Texte re√ßu: {text[:500]}")
        
        # Si pas de JSON trouv√©, retourner un dict vide
        logger.warning("Aucun JSON valide trouv√© dans la r√©ponse")
        return {}
    
    def _format_sources(self, retrieval_result: RetrievalResult) -> List[Dict]:
        """
        Formate les sources pour compatibilit√© avec l'ancien format
        """
        sources = []
        seen_sources = set()  # Pour √©viter les doublons
        
        for source in retrieval_result.sources[:3]:  # Max 3 sources
            source_key = f"{source.get('document')}_{source.get('page')}"
            
            if source_key not in seen_sources:
                sources.append({
                    'document': source.get('document', 'Unknown'),
                    'section': source.get('section', '')
                })
                seen_sources.add(source_key)
        
        return sources
    
    def _generate_summary(self, questions: Dict[str, Dict]) -> Dict:
        """
        G√©n√®re un r√©sum√© simple de l'identit√© extraite
        Inclut maintenant un indicateur de qualit√© bas√© sur les evidence_chunks
        """
        summary = {}
        
        # Mapping des questions aux cl√©s du r√©sum√©
        mapping = {
            "identity.client_name": "client",
            "identity.tender_reference": "reference",
            "identity.timeline_milestones": "deadline",
            "identity.submission_channel": "submission",
            "identity.expected_deliverables": "deliverables",
            "identity.operating_countries": "countries",
            "identity.service_main_scope": "scope",
            "identity.contract_type": "contract_type",
            "identity.contract_duration": "duration"
        }
        
        for question_id, key in mapping.items():
            if question_id in questions:
                result = questions[question_id]
                if result['status'] in ['success', 'partial']:
                    # Ajouter la valeur extraite
                    summary[key] = result['answer']
                    
                    # NOUVEAU: Ajouter un indicateur de qualit√© bas√© sur les evidence
                    if 'evidence_chunks' in result and result['evidence_chunks']:
                        summary[f"{key}_quality"] = {
                            'confidence': result['confidence'],
                            'sources_count': len(result['evidence_chunks']),
                            'status': result['status']
                        }
                else:
                    summary[key] = None
                    summary[f"{key}_quality"] = {
                        'confidence': 0.0,
                        'sources_count': 0,
                        'status': 'failed'
                    }
        
        return summary
    
    def get_extraction_report(self, results: Dict) -> str:
        """
        G√©n√®re un rapport textuel d√©taill√© de l'extraction
        NOUVEAU: m√©thode helper pour affichage
        """
        report_lines = []
        report_lines.append("\n" + "="*60)
        report_lines.append("üìä RAPPORT D'EXTRACTION D'IDENTIT√â")
        report_lines.append("="*60)
        
        # R√©sum√© des r√©ponses
        report_lines.append("\nüìã INFORMATIONS EXTRAITES:")
        report_lines.append("-" * 40)
        
        for key, value in results['summary'].items():
            if not key.endswith('_quality'):
                quality = results['summary'].get(f"{key}_quality", {})
                if value:
                    confidence = quality.get('confidence', 0) * 100
                    sources = quality.get('sources_count', 0)
                    report_lines.append(
                        f"‚úÖ {key.upper()}: {value} "
                        f"(Confiance: {confidence:.0f}%, Sources: {sources})"
                    )
                else:
                    report_lines.append(f"‚ùå {key.upper()}: Non trouv√©")
        
        # Documents r√©f√©renc√©s
        if 'metadata' in results:
            report_lines.append(f"\nüìö DOCUMENTS ANALYS√âS:")
            report_lines.append("-" * 40)
            for doc in results['metadata']['documents_referenced']:
                report_lines.append(f"   ‚Ä¢ {doc}")
            
            report_lines.append(
                f"\nüìÑ Total chunks analys√©s: {results['metadata']['total_chunks_analyzed']}"
            )
        
        # Statistiques
        if 'stats' in results:
            stats = results['stats']
            report_lines.append(f"\nüìä STATISTIQUES:")
            report_lines.append("-" * 40)
            report_lines.append(f"   Questions trait√©es: {stats['total']}")
            report_lines.append(f"   Succ√®s: {stats['success']}")
            report_lines.append(f"   √âchecs: {stats['failed']}")
            report_lines.append(f"   Chunks utilis√©s: {stats.get('total_chunks_used', 0)}")
            report_lines.append(f"   Temps: {stats['time_seconds']:.1f}s")
        
        return "\n".join(report_lines)
    
    def clear_cache(self):
        """
        Vide le cache des embeddings (utile pour forcer le recalcul)
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        if cache_file.exists():
            cache_file.unlink()
            logger.info("üóëÔ∏è Cache supprim√©")
        
        # R√©initialiser
        self.embeddings_cache = self._initialize_embeddings_cache()


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """Test de l'agent am√©lior√© avec tra√ßabilit√© compl√®te et cache optimis√©"""
    import os
    
    # V√©rifier la cl√© API
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå OPENAI_API_KEY non trouv√©e")
        print("   D√©finissez-la avec: export OPENAI_API_KEY='votre-cl√©'")
        return
    
    print("\n" + "="*60)
    print("ü§ñ AGENT D'IDENTIT√â AVEC TRA√áABILIT√â COMPL√àTE ET CACHE")
    print("="*60)
    
    # Imports
    from utils.vectorstore import IntelligentVectorStore
    from utils.enhanced_retrieval import IntelligentRetriever
    
    # 1. Charger le vectorstore
    print("\nüìö Chargement du vectorstore...")
    vectorstore = IntelligentVectorStore(
        persist_directory=Path("data/intelligent_store"),
        llm_model="gpt-4o-mini"
    )
    stats = vectorstore.get_stats()
    print(f"‚úÖ Vectorstore: {stats['total_documents']} documents")
    
    if stats['total_documents'] == 0:
        print("‚ùå Vectorstore vide! Lancez d'abord la vectorisation.")
        print("   Commande: python utils/vectorize_documents.py --source <dossier>")
        return
    
    # 2. Cr√©er le retriever
    print("\nüîç Initialisation du retriever...")
    retriever = IntelligentRetriever(
        vectorstore=vectorstore,
        llm_model="gpt-4o-mini"
    )
    
    # 3. Cr√©er l'agent
    print("\nü§ñ Cr√©ation de l'agent avec cache intelligent...")
    agent = SimpleIdentityAgent(
        vectorstore=vectorstore,
        retriever=retriever,
        config_path="configs/prompts/identity/en.yaml"
    )
    
    # NOUVEAU : Afficher l'√©tat du cache au d√©marrage
    if hasattr(agent, 'embeddings_cache'):
        print(f"üíæ Cache d'embeddings: {len(agent.embeddings_cache)} queries pr√©-calcul√©es")
    
    # 4. Lancer l'extraction
    print("\nüöÄ Lancement de l'extraction avec tra√ßabilit√©...")
    start_time = datetime.now()
    results = agent.extract_all()
    elapsed_time = (datetime.now() - start_time).total_seconds()
    
    # 5. Afficher le rapport d√©taill√©
    report = agent.get_extraction_report(results)
    print(report)
    
    # NOUVEAU : Section d√©di√©e aux statistiques de cache
    print("\n" + "="*60)
    print("üíæ PERFORMANCE DU CACHE")
    print("="*60)
    
    if 'stats' in results:
        stats = results['stats']
        
        # Calculer les m√©triques du cache
        cache_hits = stats.get('cache_hits', 0)
        cache_misses = stats.get('cache_misses', 0)
        total_cache_ops = cache_hits + cache_misses
        
        if total_cache_ops > 0:
            cache_hit_ratio = (cache_hits / total_cache_ops) * 100
            
            print(f"üìä Statistiques du Cache:")
            print(f"   ‚îú‚îÄ Cache hits: {cache_hits}")
            print(f"   ‚îú‚îÄ Cache misses: {cache_misses}")
            print(f"   ‚îú‚îÄ Hit ratio: {cache_hit_ratio:.1f}%")
            print(f"   ‚îî‚îÄ Total op√©rations: {total_cache_ops}")
            
            # Calculer les √©conomies
            embedding_cost_per_call = 0.00002  # Co√ªt approximatif par embedding
            saved_embedding_calls = cache_hits
            saved_cost = saved_embedding_calls * embedding_cost_per_call
            time_saved = saved_embedding_calls * 0.1  # ~100ms par appel API √©vit√©
            
            print(f"\nüí∞ √âconomies R√©alis√©es:")
            print(f"   ‚îú‚îÄ Appels API √©vit√©s: {saved_embedding_calls}")
            print(f"   ‚îú‚îÄ Co√ªt √©conomis√©: ~${saved_cost:.5f}")
            print(f"   ‚îî‚îÄ Temps gagn√©: ~{time_saved:.1f}s")
            
            # Comparaison avec/sans cache
            print(f"\nüìà Comparaison Performance:")
            print(f"   ‚îú‚îÄ Temps total avec cache: {elapsed_time:.1f}s")
            print(f"   ‚îú‚îÄ Temps estim√© sans cache: ~{elapsed_time + time_saved:.1f}s")
            print(f"   ‚îî‚îÄ Am√©lioration: {(time_saved/(elapsed_time + time_saved)*100):.0f}% plus rapide")
        else:
            print("‚ö†Ô∏è Aucune statistique de cache disponible")
    
    # 6. Afficher un exemple de tra√ßabilit√© d√©taill√©e
    print("\n" + "="*60)
    print("üîç EXEMPLE DE TRA√áABILIT√â D√âTAILL√âE")
    print("="*60)
    
    # Prendre la premi√®re question r√©ussie comme exemple
    example_shown = False
    for question_id, result in results['questions'].items():
        if result['status'] == 'success' and result.get('evidence_chunks'):
            print(f"\nüìå Question: {question_id}")
            print(f"üìù R√©ponse: {result['answer']}")
            print(f"üéØ Confiance: {result['confidence']*100:.0f}%")
            
            # NOUVEAU : Indiquer si la r√©ponse vient du cache
            if result.get('metadata', {}).get('from_cache'):
                print(f"üíæ Source: Cache (embedding pr√©-calcul√©)")
            
            print(f"\nüìÑ Evidence Chunks ({len(result['evidence_chunks'])} chunks utilis√©s):")
            
            for i, chunk in enumerate(result['evidence_chunks'], 1):
                print(f"\n   Chunk {i}:")
                print(f"   ‚îú‚îÄ Source: {chunk['metadata']['source']}")
                print(f"   ‚îú‚îÄ Section: {chunk['metadata']['section']}")
                print(f"   ‚îú‚îÄ Score: {chunk['relevance_score']:.2f}")
                print(f"   ‚îî‚îÄ Extrait: {chunk['text_full'][:150]}...")
            
            example_shown = True
            break  # Afficher juste un exemple
    
    if not example_shown:
        print("‚ÑπÔ∏è Aucun exemple de tra√ßabilit√© disponible")
    
    # 7. Optionnel: Sauvegarder les r√©sultats
    output_file = Path("extraction_results_with_evidence.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ R√©sultats sauvegard√©s dans: {output_file}")
    
    # NOUVEAU : R√©sum√© final avec m√©triques de performance
    print("\n" + "="*60)
    print("‚úÖ EXTRACTION TERMIN√âE AVEC SUCC√àS!")
    print("="*60)
    
    print(f"\nüìä R√©sum√© Final:")
    print(f"   ‚îú‚îÄ Questions trait√©es: {stats.get('total', 0)}")
    print(f"   ‚îú‚îÄ Succ√®s: {stats.get('success', 0)}/{stats.get('total', 0)}")
    print(f"   ‚îú‚îÄ Temps total: {elapsed_time:.1f}s")
    
    if total_cache_ops > 0:
        print(f"   ‚îú‚îÄ Cache utilis√©: {cache_hit_ratio:.0f}% des requ√™tes")
        print(f"   ‚îî‚îÄ Efficacit√©: ${saved_cost:.5f} √©conomis√©s")
    else:
        print(f"   ‚îî‚îÄ Cache: Non utilis√©")
    
    # NOUVEAU : Suggestion d'optimisation
    if cache_misses > 0:
        print(f"\nüí° Conseil: {cache_misses} requ√™tes n'√©taient pas dans le cache.")
        print(f"   Relancez l'extraction pour b√©n√©ficier du cache √† 100%!")


if __name__ == "__main__":
    main()