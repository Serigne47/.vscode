# agents/execution_agent.py
"""
Agent d'ex√©cution pour l'analyse des aspects op√©rationnels des appels d'offres
Version adapt√©e de l'Identity Agent avec support GPT-5-mini
"""

import logging
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict, field
import yaml
import pickle
import hashlib

import sys
sys.path.append(str(Path(__file__).parent.parent))

from openai import OpenAI

# Import des composants
from utils.vectorstore import IntelligentVectorStore
from utils.enhanced_retrieval import IntelligentRetriever, RetrievalResult

logger = logging.getLogger(__name__)

@dataclass
class ExecutionQuestion:
    """Structure d'une question d'ex√©cution"""
    id: str
    system_prompt: str
    validator: Dict[str, Any] = None
    
@dataclass
class ExecutionAnswer:
    """
    R√©ponse structur√©e avec tra√ßabilit√© compl√®te
    Inclut maintenant les chunks complets utilis√©s pour l'extraction
    """
    question_id: str
    answer: Any
    sources: List[Dict[str, str]]
    confidence: float
    status: str  # success, partial, failed
    evidence_chunks: List[Dict[str, Any]] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        """Convertit en dictionnaire avec tous les champs"""
        return asdict(self)


class ExecutionAgent:
    """
    Agent d'extraction des informations d'ex√©cution op√©rationnelle
    Version adapt√©e pour GPT-5-mini avec tra√ßabilit√© compl√®te
    """
    
    def __init__(
        self,
        vectorstore: IntelligentVectorStore,
        retriever: IntelligentRetriever,
        config_path: str = "configs/prompts/execution/en.yaml"
    ):
        """
        Initialise l'agent d'ex√©cution
        
        Args:
            vectorstore: IntelligentVectorStore configur√©
            retriever: IntelligentRetriever configur√©
            config_path: Chemin vers le YAML des questions d'ex√©cution
        """
        self.vectorstore = vectorstore
        self.retriever = retriever
        
        # Client OpenAI direct pour GPT-5-mini
        self.client = OpenAI()
        self.model = "gpt-5-mini"
        
        # Charger les questions d'ex√©cution
        self.questions = self._load_questions(config_path)

        # Initialiser le syst√®me de cache
        self.cache_dir = Path("data/cache/execution_agent")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
        # Queries optimis√©es pour chaque question d'ex√©cution
        self.optimized_queries = {
            "execution.transport_mode": "transport mode air sea ocean road rail multimodal freight shipping logistics",
            "execution.expected_services": "services freight forwarding customs clearance warehousing delivery logistics operations",
            "execution.operational_requirements": "requirements temperature GPS tracking EDI certification ISO IATA documentation",
            "execution.sla_kpi": "SLA KPI performance indicators on-time delivery rate metrics targets"
        }
    
        # Pr√©-charger ou calculer les embeddings
        self.embeddings_cache = self._initialize_embeddings_cache()
        
        # Stats basiques
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'total_chunks_used': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        logger.info(f"‚úÖ Execution Agent initialis√© avec {len(self.questions)} questions")
        logger.info(f"üíæ Cache: {len(self.embeddings_cache)} embeddings pr√©-calcul√©s")

    def _initialize_embeddings_cache(self) -> Dict[str, Any]:
        """
        Initialise le cache d'embeddings persistant
        Charge depuis le disque ou calcule si n√©cessaire
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        
        # Essayer de charger le cache existant
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cache = pickle.load(f)
                    logger.info(f"‚úÖ Cache charg√© depuis {cache_file}")
                    
                    # V√©rifier que toutes les questions sont dans le cache
                    missing = set(self.optimized_queries.keys()) - set(cache.keys())
                    if missing:
                        logger.info(f"‚ö†Ô∏è Questions manquantes dans le cache: {missing}")
                        for question_id in missing:
                            cache = self._add_to_cache(cache, question_id)
                        self._save_cache(cache)
                    
                    return cache
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de charger le cache: {e}")
        
        # Si pas de cache, le cr√©er
        logger.info("üîÑ Cr√©ation du cache d'embeddings...")
        cache = {}
        
        for question_id, query in self.optimized_queries.items():
            cache = self._add_to_cache(cache, question_id)
        
        # Sauvegarder le cache
        self._save_cache(cache)
        logger.info(f"‚úÖ Cache cr√©√© avec {len(cache)} embeddings")
        
        return cache

    def _add_to_cache(self, cache: Dict, question_id: str) -> Dict:
        """
        Ajoute un embedding au cache
        """
        query = self.optimized_queries.get(question_id)
        if query:
            logger.info(f"  üìù Calcul embedding pour: {question_id}")
            embedding = self.vectorstore.embeddings.embed_query(query)
            cache[question_id] = {
                'query': query,
                'embedding': embedding,
                'created_at': datetime.now().isoformat()
            }
        return cache

    def _save_cache(self, cache: Dict):
        """
        Sauvegarde le cache sur disque
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(cache, f)
        logger.info(f"üíæ Cache sauvegard√©: {cache_file}")
    
    def _load_questions(self, config_path: str) -> List[ExecutionQuestion]:
        """Charge les questions depuis le YAML"""
        questions = []
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            for item in config:
                question = ExecutionQuestion(
                    id=item['id'],
                    system_prompt=item['system_prompt'],
                    validator=item.get('validator', {})
                )
                questions.append(question)
            
            logger.info(f"üìã {len(questions)} questions d'ex√©cution charg√©es")
            
        except Exception as e:
            logger.error(f"Erreur chargement config: {e}")
            raise
        
        return questions
    
    def extract_all(self) -> Dict[str, Any]:
        """
        Extrait toutes les informations d'ex√©cution avec tra√ßabilit√© compl√®te
        
        Returns:
            Dictionnaire avec toutes les r√©ponses et leurs sources d√©taill√©es
        """
        logger.info("\n" + "="*60)
        logger.info("‚öôÔ∏è EXTRACTION DES INFORMATIONS D'EX√âCUTION")
        logger.info("="*60)
        
        start_time = datetime.now()
        
        results = {
            'timestamp': start_time.isoformat(),
            'questions': {},
            'summary': {},
            'metadata': {
                'total_chunks_analyzed': 0,
                'documents_referenced': set()
            }
        }
        
        # Traiter chaque question
        for question in self.questions:
            answer = self.extract_single(question)
            results['questions'][answer.question_id] = answer.to_dict()
            
            # Collecter les m√©tadonn√©es globales
            for chunk in answer.evidence_chunks:
                if 'metadata' in chunk and 'source' in chunk['metadata']:
                    results['metadata']['documents_referenced'].add(
                        chunk['metadata']['source']
                    )
            results['metadata']['total_chunks_analyzed'] += len(answer.evidence_chunks)
        
        # Convertir le set en liste pour la s√©rialisation JSON
        results['metadata']['documents_referenced'] = list(
            results['metadata']['documents_referenced']
        )
        
        # G√©n√©rer le r√©sum√©
        results['summary'] = self._generate_summary(results['questions'])
        
        # Stats finales
        elapsed = (datetime.now() - start_time).total_seconds()
        results['stats'] = {
            **self.stats,
            'time_seconds': elapsed,
            'avg_chunks_per_question': (
                self.stats['total_chunks_used'] / self.stats['total'] 
                if self.stats['total'] > 0 else 0
            )
        }
        
        logger.info(f"\n‚úÖ Extraction termin√©e en {elapsed:.1f}s")
        logger.info(f"   Succ√®s: {self.stats['success']}/{self.stats['total']}")
        logger.info(f"   Chunks utilis√©s: {self.stats['total_chunks_used']}")
        
        return results
    
    def extract_single(self, question: ExecutionQuestion) -> ExecutionAnswer:
        """
        Extrait l'information pour une question avec cache optimis√©
        
        Args:
            question: Question √† traiter
            
        Returns:
            ExecutionAnswer avec la r√©ponse et tous les chunks utilis√©s
        """
        self.stats['total'] += 1
        logger.info(f"\n‚öôÔ∏è Traitement: {question.id}")
        
        try:
            # Utiliser le cache si disponible
            if question.id in self.embeddings_cache:
                retrieval_result = self._retrieve_with_cached_embedding(
                    question_id=question.id,
                    category="execution"
                )
                self.stats['cache_hits'] += 1
                logger.debug(f"   ‚úÖ Cache hit pour {question.id}")
            else:
                search_query = self._create_search_query(question)
                retrieval_result = self.retriever.retrieve_and_answer(
                    query=search_query,
                    category="execution",
                    require_source=True,
                    max_chunks=3
                )
                self.stats['cache_misses'] += 1
                logger.debug(f"   ‚ö†Ô∏è Cache miss pour {question.id}")
            
            # Extraire la r√©ponse structur√©e avec les chunks complets
            answer = self._extract_answer_with_evidence(question, retrieval_result)
            
            # Mise √† jour des stats
            if answer.status == "success":
                self.stats['success'] += 1
            else:
                self.stats['failed'] += 1
            
            self.stats['total_chunks_used'] += len(answer.evidence_chunks)
            
            logger.info(f"   ‚úÖ Status: {answer.status} (confiance: {answer.confidence:.0%})")
            logger.info(f"   üìÑ Chunks utilis√©s: {len(answer.evidence_chunks)}")
            
            return answer
            
        except Exception as e:
            logger.error(f"   ‚ùå Erreur: {e}")
            self.stats['failed'] += 1
            
            return ExecutionAnswer(
                question_id=question.id,
                answer=None,
                sources=[],
                confidence=0.0,
                status="failed",
                evidence_chunks=[]
            )
    
    def _retrieve_with_cached_embedding(
        self,
        question_id: str,
        category: str
    ) -> RetrievalResult:
        """
        Effectue une recherche en utilisant l'embedding pr√©-calcul√©
        """
        cached_data = self.embeddings_cache[question_id]
        query = cached_data['query']
        embedding = cached_data['embedding']
        
        results = self._search_vectorstore_with_embedding(
            embedding=embedding,
            k=3
        )
        
        chunks = []
        sources = []
        
        for result in results:
            chunks.append(result)
            if 'source' in result:
                sources.append(result['source'])
        
        answer_text = f"Information found in {len(chunks)} chunks"
        
        return RetrievalResult(
            query=query,
            answer=answer_text,
            chunks=chunks,
            sources=sources[:3],
            confidence=0.8 if chunks else 0.0,
            metadata={'from_cache': True}
        )

    def _search_vectorstore_with_embedding(
        self,
        embedding: List[float],
        k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        Recherche directe dans FAISS avec un embedding pr√©-calcul√©
        """
        import numpy as np
        
        query_vector = np.array([embedding], dtype=np.float32)
        
        if hasattr(self.vectorstore, 'index') and self.vectorstore.index:
            import faiss
            faiss.normalize_L2(query_vector)
            
            distances, indices = self.vectorstore.index.search(query_vector, k)
            
            results = []
            for idx, dist in zip(indices[0], distances[0]):
                if idx >= 0 and idx in self.vectorstore.id_to_text:
                    chunk_result = {
                        'text': self.vectorstore.id_to_text[idx],
                        'metadata': self.vectorstore.id_to_metadata.get(idx, {}),
                        'score': float(dist),
                        'source': self.vectorstore.id_to_metadata.get(idx, {})
                    }
                    results.append(chunk_result)
            
            return results
        else:
            logger.warning("Pas d'acc√®s direct √† FAISS, utilisation de la recherche normale")
            return []
    
    def _create_search_query(self, question: ExecutionQuestion) -> str:
        """
        Retourne la query optimis√©e depuis le mapping
        """
        return self.optimized_queries.get(
            question.id, 
            "tender execution operational requirements"
        )
    
    def _extract_answer_with_evidence(
        self,
        question: ExecutionQuestion,
        retrieval_result: RetrievalResult
    ) -> ExecutionAnswer:
        """
        Extrait la r√©ponse depuis les chunks trouv√©s avec tra√ßabilit√© compl√®te
        Adapt√© pour la nouvelle structure JSON avec answer.result et answer.sources
        """
        # Pr√©parer le contexte ET capturer les chunks info
        context, chunks_details = self._prepare_context_with_tracking(
            retrieval_result.chunks[:3]
        )
        
        # Construire evidence_chunks avec toutes les m√©tadonn√©es
        evidence_chunks = self._build_evidence_chunks(
            retrieval_result.chunks[:3],
            chunks_details
        )
        
        # Prompt pour extraction JSON
        prompt_text = f"""{question.system_prompt}

Context from documents:
{context}

IMPORTANT: Return ONLY a valid JSON object. No explanations.
The JSON must start with {{ and end with }}.
"""
        
        try:
            # Appeler GPT-5-mini avec le contexte COMPLET
            response = self.client.responses.create(
                model=self.model,
                input=prompt_text,
                reasoning={"effort": "minimal"},
                text={"verbosity": "low"}
            )
            result_text = response.output_text.strip()
            
            # Extraire le JSON de la r√©ponse
            json_obj = self._extract_json(result_text)
            
            # La nouvelle structure a answer.result et answer.sources
            answer_data = json_obj.get('answer', {})
            
            # Cr√©er la r√©ponse enrichie
            return ExecutionAnswer(
                question_id=question.id,
                answer=answer_data,  # Contient {"result": ..., "sources": [...]}
                sources=[],
                confidence=self._calculate_confidence(answer_data, retrieval_result),
                status=self._determine_status(answer_data),
                evidence_chunks=evidence_chunks
            )
            
        except Exception as e:
            logger.debug(f"Erreur extraction: {e}")
            
            # Fallback
            return ExecutionAnswer(
                question_id=question.id,
                answer=None,
                sources=[],
                confidence=retrieval_result.confidence * 0.5,
                status="partial",
                evidence_chunks=evidence_chunks
            )
    
    def _prepare_context_with_tracking(
        self, 
        chunks: List[Dict]
    ) -> Tuple[str, List[Dict]]:
        """
        Pr√©pare le contexte depuis les chunks avec TEXTE COMPLET
        """
        context_parts = []
        chunks_details = []
        
        for i, chunk in enumerate(chunks):
            source = chunk.get('source', {})
            metadata = chunk.get('metadata', {})
            
            text_full = chunk.get('text', '')
            
            # Inclure le nom du document pour que le LLM puisse le r√©f√©rencer
            doc_name = source.get('document', 'Unknown')
            section = source.get('section', 'N/A')
            
            # Format pour le LLM
            context_parts.append(
                f"[Document: {doc_name}]\n"
                f"[Section: {section}]\n"
                f"{text_full}"
            )
            
            chunks_details.append({
                'index': i,
                'text': text_full,
                'source': doc_name,
                'section': section,
                'metadata': metadata
            })
        
        context_string = "\n---\n".join(context_parts)
        
        logger.debug(f"   Context pr√©par√©: {len(context_string)} caract√®res depuis {len(chunks)} chunks")
        
        return context_string, chunks_details
    
    def _build_evidence_chunks(
        self,
        original_chunks: List[Dict],
        chunks_details: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        Construit la structure evidence_chunks avec m√©tadonn√©es essentielles
        """
        evidence_chunks = []
        
        for chunk, details in zip(original_chunks, chunks_details):
            evidence_chunk = {
                "chunk_index": details['index'],
                "text_full": details['text'],
                "metadata": {
                    "source": details['source'],
                    "section": details['section']
                },
                "relevance_score": chunk.get('score', 0.0) if 'score' in chunk else chunk.get('confidence', 0.0)
            }
            
            evidence_chunks.append(evidence_chunk)
        
        return evidence_chunks
    
    def _extract_json(self, text: str) -> Dict:
        """
        Extrait un objet JSON depuis du texte
        """
        text = text.strip()
        
        if '{' in text and '}' in text:
            start = text.find('{')
            end = text.rfind('}') + 1
            json_str = text[start:end]
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.debug(f"Premi√®re tentative JSON √©chou√©e: {e}")
                json_str = json_str.replace('\n', ' ').replace('\r', '')
                json_str = ' '.join(line.split('//')[0] for line in json_str.split('\n'))
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as e2:
                    logger.error(f"Impossible d'extraire JSON: {e2}")
                    logger.debug(f"Texte re√ßu: {text[:500]}")
        
        logger.warning("Aucun JSON valide trouv√© dans la r√©ponse")
        return {}
    
    def _format_sources_from_answer(
        self, 
        answer_data: Dict, 
        retrieval_result: RetrievalResult
    ) -> List[Dict]:
        """
        Formate les sources depuis la r√©ponse du LLM ou du retrieval
        """
        # Si le LLM a fourni des sources dans sa r√©ponse
        if answer_data and 'sources' in answer_data:
            return answer_data['sources'][:3]
        
        # Sinon, utiliser les sources du retrieval
        return self._format_sources(retrieval_result)
    
    def _format_sources(self, retrieval_result: RetrievalResult) -> List[Dict]:
        """
        Formate les sources depuis le retrieval result
        """
        sources = []
        seen_sources = set()
        
        for source in retrieval_result.sources[:3]:
            source_key = f"{source.get('document')}_{source.get('section')}"
            
            if source_key not in seen_sources:
                sources.append({
                    'document': source.get('document', 'Unknown'),
                    'section': source.get('section', '')
                })
                seen_sources.add(source_key)
        
        return sources
    
    def _calculate_confidence(
        self, 
        answer_data: Dict, 
        retrieval_result: RetrievalResult
    ) -> float:
        """
        Calcule la confiance bas√©e sur la r√©ponse et les sources
        """
        if not answer_data or not answer_data.get('result'):
            return 0.0
        
        base_confidence = retrieval_result.confidence
        
        # Augmenter si des sources sont fournies
        if answer_data.get('sources'):
            base_confidence = min(1.0, base_confidence * 1.1)
        
        return base_confidence
    
    def _determine_status(self, answer_data: Dict) -> str:
        """
        D√©termine le statut bas√© sur la compl√©tude de la r√©ponse
        """
        if not answer_data:
            return "failed"
        
        result = answer_data.get('result')
        sources = answer_data.get('sources', [])
        
        if result and sources:
            return "success"
        elif result:
            return "partial"
        else:
            return "failed"
    
    def _generate_summary(self, questions: Dict[str, Dict]) -> Dict:
        """
        G√©n√®re un r√©sum√© des informations d'ex√©cution extraites
        """
        summary = {}
        
        # Mapping des questions aux cl√©s du r√©sum√©
        mapping = {
            "execution.transport_mode": "transport_modes",
            "execution.expected_services": "services",
            "execution.operational_requirements": "requirements",
            "execution.sla_kpi": "sla_kpis"
        }
        
        for question_id, key in mapping.items():
            if question_id in questions:
                result = questions[question_id]
                if result['status'] in ['success', 'partial']:
                    # Extraire la valeur depuis answer.result
                    answer_data = result.get('answer', {})
                    if isinstance(answer_data, dict):
                        summary[key] = answer_data.get('result')
                    else:
                        summary[key] = answer_data
                    
                    # Ajouter indicateur de qualit√©
                    if 'evidence_chunks' in result and result['evidence_chunks']:
                        summary[f"{key}_quality"] = {
                            'confidence': result['confidence'],
                            'sources_count': len(result['evidence_chunks']),
                            'status': result['status']
                        }
                else:
                    summary[key] = None
                    summary[f"{key}_quality"] = {
                        'confidence': 0.0,
                        'sources_count': 0,
                        'status': 'failed'
                    }
        
        return summary
    
    def get_extraction_report(self, results: Dict) -> str:
        """
        G√©n√®re un rapport textuel d√©taill√© de l'extraction
        """
        report_lines = []
        report_lines.append("\n" + "="*60)
        report_lines.append("‚öôÔ∏è RAPPORT D'EXTRACTION D'EX√âCUTION")
        report_lines.append("="*60)
        
        # R√©sum√© des r√©ponses
        report_lines.append("\nüìã INFORMATIONS D'EX√âCUTION EXTRAITES:")
        report_lines.append("-" * 40)
        
        for key, value in results['summary'].items():
            if not key.endswith('_quality'):
                quality = results['summary'].get(f"{key}_quality", {})
                if value:
                    confidence = quality.get('confidence', 0) * 100
                    sources = quality.get('sources_count', 0)
                    
                    # Formater selon le type de valeur
                    if isinstance(value, list):
                        value_str = ', '.join(value) if value else 'None'
                    else:
                        value_str = str(value)
                    
                    report_lines.append(
                        f"‚úÖ {key.upper()}: {value_str} "
                        f"(Confiance: {confidence:.0f}%, Sources: {sources})"
                    )
                else:
                    report_lines.append(f"‚ùå {key.upper()}: Non trouv√©")
        
        # Documents r√©f√©renc√©s
        if 'metadata' in results:
            report_lines.append(f"\nüìö DOCUMENTS ANALYS√âS:")
            report_lines.append("-" * 40)
            for doc in results['metadata']['documents_referenced']:
                report_lines.append(f"   ‚Ä¢ {doc}")
            
            report_lines.append(
                f"\nüìÑ Total chunks analys√©s: {results['metadata']['total_chunks_analyzed']}"
            )
        
        # Statistiques
        if 'stats' in results:
            stats = results['stats']
            report_lines.append(f"\nüìä STATISTIQUES:")
            report_lines.append("-" * 40)
            report_lines.append(f"   Questions trait√©es: {stats['total']}")
            report_lines.append(f"   Succ√®s: {stats['success']}")
            report_lines.append(f"   √âchecs: {stats['failed']}")
            report_lines.append(f"   Chunks utilis√©s: {stats.get('total_chunks_used', 0)}")
            report_lines.append(f"   Temps: {stats['time_seconds']:.1f}s")
        
        return "\n".join(report_lines)
    
    def clear_cache(self):
        """
        Vide le cache des embeddings
        """
        cache_file = self.cache_dir / "embeddings_cache.pkl"
        if cache_file.exists():
            cache_file.unlink()
            logger.info("üóëÔ∏è Cache supprim√©")
        
        # R√©initialiser
        self.embeddings_cache = self._initialize_embeddings_cache()


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """Test de l'agent d'ex√©cution avec tra√ßabilit√© compl√®te"""
    import os
    
    # V√©rifier la cl√© API
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå OPENAI_API_KEY non trouv√©e")
        print("   D√©finissez-la avec: export OPENAI_API_KEY='votre-cl√©'")
        return
    
    print("\n" + "="*60)
    print("‚öôÔ∏è AGENT D'EX√âCUTION AVEC TRA√áABILIT√â COMPL√àTE")
    print("="*60)
    
    # Imports
    from utils.vectorstore import IntelligentVectorStore
    from utils.enhanced_retrieval import IntelligentRetriever
    
    # 1. Charger le vectorstore
    print("\nüìö Chargement du vectorstore...")
    vectorstore = IntelligentVectorStore(
        persist_directory=Path("data/intelligent_store"),
        llm_model="gpt-4o-mini"
    )
    stats = vectorstore.get_stats()
    print(f"‚úÖ Vectorstore: {stats['total_documents']} documents")
    
    if stats['total_documents'] == 0:
        print("‚ùå Vectorstore vide! Lancez d'abord la vectorisation.")
        print("   Commande: python utils/vectorize_documents.py --source <dossier>")
        return
    
    # 2. Cr√©er le retriever
    print("\nüîç Initialisation du retriever...")
    retriever = IntelligentRetriever(
        vectorstore=vectorstore,
        llm_model="gpt-4o-mini"
    )
    
    # 3. Cr√©er l'agent d'ex√©cution
    print("\n‚öôÔ∏è Cr√©ation de l'agent d'ex√©cution...")
    agent = ExecutionAgent(
        vectorstore=vectorstore,
        retriever=retriever,
        config_path="configs/prompts/execution/en.yaml"
    )
    
    # Afficher l'√©tat du cache
    if hasattr(agent, 'embeddings_cache'):
        print(f"üíæ Cache d'embeddings: {len(agent.embeddings_cache)} queries pr√©-calcul√©es")
    
    # 4. Lancer l'extraction
    print("\nüöÄ Lancement de l'extraction d'ex√©cution...")
    start_time = datetime.now()
    results = agent.extract_all()
    elapsed_time = (datetime.now() - start_time).total_seconds()
    
    # 5. Afficher le rapport d√©taill√©
    report = agent.get_extraction_report(results)
    print(report)
    
    # 6. Performance du cache
    print("\n" + "="*60)
    print("üíæ PERFORMANCE DU CACHE")
    print("="*60)
    
    if 'stats' in results:
        stats = results['stats']
        
        cache_hits = stats.get('cache_hits', 0)
        cache_misses = stats.get('cache_misses', 0)
        total_cache_ops = cache_hits + cache_misses
        
        if total_cache_ops > 0:
            cache_hit_ratio = (cache_hits / total_cache_ops) * 100
            
            print(f"üìä Statistiques du Cache:")
            print(f"   ‚îú‚îÄ Cache hits: {cache_hits}")
            print(f"   ‚îú‚îÄ Cache misses: {cache_misses}")
            print(f"   ‚îú‚îÄ Hit ratio: {cache_hit_ratio:.1f}%")
            print(f"   ‚îî‚îÄ Total op√©rations: {total_cache_ops}")
            
            # √âconomies
            embedding_cost_per_call = 0.00002
            saved_embedding_calls = cache_hits
            saved_cost = saved_embedding_calls * embedding_cost_per_call
            time_saved = saved_embedding_calls * 0.1
            
            print(f"\nüí∞ √âconomies R√©alis√©es:")
            print(f"   ‚îú‚îÄ Appels API √©vit√©s: {saved_embedding_calls}")
            print(f"   ‚îú‚îÄ Co√ªt √©conomis√©: ~${saved_cost:.5f}")
            print(f"   ‚îî‚îÄ Temps gagn√©: ~{time_saved:.1f}s")
    
    # 7. Exemple de tra√ßabilit√©
    print("\n" + "="*60)
    print("üîç EXEMPLE DE TRA√áABILIT√â D√âTAILL√âE")
    print("="*60)
    
    example_shown = False
    for question_id, result in results['questions'].items():
        if result['status'] == 'success' and result.get('evidence_chunks'):
            print(f"\nüìå Question: {question_id}")
            
            # Extraire la r√©ponse depuis la nouvelle structure
            answer_data = result.get('answer', {})
            if isinstance(answer_data, dict):
                answer_result = answer_data.get('result', 'N/A')
            else:
                answer_result = answer_data
            
            print(f"üìù R√©ponse: {answer_result}")
            print(f"üéØ Confiance: {result['confidence']*100:.0f}%")
            
            print(f"\nüìÑ Evidence Chunks ({len(result['evidence_chunks'])} chunks utilis√©s):")
            
            for i, chunk in enumerate(result['evidence_chunks'], 1):
                print(f"\n   Chunk {i}:")
                print(f"   ‚îú‚îÄ Source: {chunk['metadata']['source']}")
                print(f"   ‚îú‚îÄ Section: {chunk['metadata']['section']}")
                print(f"   ‚îú‚îÄ Score: {chunk['relevance_score']:.2f}")
                print(f"   ‚îî‚îÄ Extrait: {chunk['text_full'][:150]}...")
            
            example_shown = True
            break
    
    if not example_shown:
        print("‚ÑπÔ∏è Aucun exemple de tra√ßabilit√© disponible")
    
    # 8. Sauvegarder les r√©sultats
    output_file = Path("execution_results_with_evidence.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ R√©sultats sauvegard√©s dans: {output_file}")
    
    # R√©sum√© final
    print("\n" + "="*60)
    print("‚úÖ EXTRACTION D'EX√âCUTION TERMIN√âE!")
    print("="*60)
    
    print(f"\nüìä R√©sum√© Final:")
    print(f"   ‚îú‚îÄ Questions trait√©es: {stats.get('total', 0)}")
    print(f"   ‚îú‚îÄ Succ√®s: {stats.get('success', 0)}/{stats.get('total', 0)}")
    print(f"   ‚îú‚îÄ Temps total: {elapsed_time:.1f}s")
    
    if total_cache_ops > 0:
        print(f"   ‚îú‚îÄ Cache utilis√©: {cache_hit_ratio:.0f}% des requ√™tes")
        print(f"   ‚îî‚îÄ Efficacit√©: ${saved_cost:.5f} √©conomis√©s")
    else:
        print(f"   ‚îî‚îÄ Cache: Non utilis√©")


if __name__ == "__main__":
    main()