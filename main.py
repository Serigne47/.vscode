# main.py
"""
Point d'entr√©e principal pour l'analyse d'appels d'offres
"""
import asyncio
import json
import logging
from pathlib import Path
from datetime import datetime
import sys
import os

# Ajouter le r√©pertoire racine au path
sys.path.append(str(Path(__file__).resolve().parent))

from aoagentc.utils.arch0.enhanced_chunker1 import EnhancedChunker
from aoagentc.utils.arch0.enhanced_retrieval1 import EnhancedAORetriever
from aoagentc.utils.arch0.vectorstore1 import get_vectorstore
from utils.report_generator import AOReportGenerator
from agent.orchestrator import AOExtractionOrchestrator
from langchain_core.documents import Document

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'ao_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION - MODIFIEZ ICI VOS CHEMINS ET PARAM√àTRES
# ============================================================================

class Config:
    """Configuration centralis√©e de l'application"""
    
    # Chemin vers vos documents d'appel d'offres
    DOCUMENTS_PATH = Path(r"C:\Users\serigne.faye\OneDrive - MSC\Bureau\Dossier de travail\MISSIONS\B PILOTAGE PROJETS INNO\2.6 PoC Power Platform\9 Agent Analyse AO\Z DOCUMENTATION\Reckitt Tender documentation 1")
    
    # Chemin pour la base de donn√©es vectorielle
    VECTORSTORE_PATH = Path("data/chroma_db")
    
    # Chemin pour les rapports g√©n√©r√©s
    REPORTS_PATH = Path("reports")
    
    # Configuration OpenAI
    OPENAI_MODEL = "gpt-4o-mini"  # ou "gpt-3.5-turbo"
    EMBEDDING_MODEL = "text-embedding-3-small"
    
    # Configuration chunking
    CHUNK_SIZE = 1500
    CHUNK_OVERLAP = 300
    
    # Configuration analyse
    MAX_PARALLEL_AGENTS = 6  # Nombre d'agents √† ex√©cuter en parall√®le
    ENABLE_CACHE = True  # Active le cache pour √©viter de retraiter les m√™mes documents
    
    # Export des r√©sultats
    EXPORT_FORMATS = ["json", "excel", "word"]  # Formats d'export souhait√©s
    
    @classmethod
    def validate(cls):
        """Valide la configuration"""
        if not cls.DOCUMENTS_PATH.exists():
            raise ValueError(f"Le chemin des documents n'existe pas: {cls.DOCUMENTS_PATH}")
        
        # Cr√©er les dossiers s'ils n'existent pas
        cls.VECTORSTORE_PATH.mkdir(parents=True, exist_ok=True)
        cls.REPORTS_PATH.mkdir(parents=True, exist_ok=True)
        
        # V√©rifier la cl√© API OpenAI
        if not os.getenv("OPENAI_API_KEY"):
            logger.warning("‚ö†Ô∏è OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
            logger.info("D√©finissez-la avec: export OPENAI_API_KEY='your-key-here'")
        
        return True

# ============================================================================
# FONCTIONS PRINCIPALES
# ============================================================================

async def analyze_tender_documents(
    documents_path: Path = None,
    force_reindex: bool = False
) -> dict:
    """
    Analyse compl√®te des documents d'appel d'offres
    
    Args:
        documents_path: Chemin vers les documents (utilise Config par d√©faut)
        force_reindex: Force la r√©indexation m√™me si les documents sont d√©j√† dans la base
    
    Returns:
        Rapport d'analyse complet
    """
    try:
        # Utiliser le chemin de Config si non sp√©cifi√©
        if documents_path is None:
            documents_path = Config.DOCUMENTS_PATH
        
        logger.info("="*80)
        logger.info("üöÄ D√âMARRAGE DE L'ANALYSE D'APPEL D'OFFRES")
        logger.info(f"üìÅ Dossier source: {documents_path}")
        logger.info("="*80)
        
        # Phase 1: Chargement et chunking des documents
        logger.info("\nüìÑ PHASE 1: Chargement des documents...")
        chunker = EnhancedChunker()
        documents = []
        
        # Parcourir tous les fichiers du dossier
        supported_extensions = {'.pdf', '.docx', '.doc', '.xlsx', '.xls'}
        files_processed = 0
        
        for file_path in documents_path.rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                logger.info(f"  ‚Üí Traitement de: {file_path.name}")
                try:
                    # Extraction avec structure
                    structured_elements = chunker.extract_with_structure(file_path)
                    
                    # Conversion en Documents LangChain
                    for element in structured_elements:
                        doc = Document(
                            page_content=element.get('text', ''),
                            metadata={
                                'source': file_path.name,
                                'type': element.get('type'),
                                'section': element.get('section'),
                                'page': element.get('page'),
                                'file_path': str(file_path)
                            }
                        )
                        documents.append(doc)
                    
                    files_processed += 1
                    logger.info(f"    ‚úì {len(structured_elements)} √©l√©ments extraits")
                    
                except Exception as e:
                    logger.error(f"    ‚úó Erreur: {e}")
        
        if files_processed == 0:
            raise ValueError(f"Aucun document trouv√© dans {documents_path}")
        
        logger.info(f"\n‚úÖ {files_processed} fichiers trait√©s, {len(documents)} chunks cr√©√©s")
        
        # Phase 2: Indexation vectorielle (si n√©cessaire)
        logger.info("\nüóÑÔ∏è PHASE 2: Indexation vectorielle...")
        vectorstore = get_vectorstore(Config.VECTORSTORE_PATH)
        
        if force_reindex or not Config.VECTORSTORE_PATH.exists():
            logger.info("  ‚Üí Indexation des documents...")
            vectorstore.add_documents(documents)
            logger.info("  ‚úì Indexation termin√©e")
        else:
            logger.info("  ‚Üí Utilisation de l'index existant")
        
        # Phase 3: Analyse multi-agent
        logger.info("\nü§ñ PHASE 3: Analyse multi-agent...")
        orchestrator = AOExtractionOrchestrator(
            vectorstore=vectorstore,
            config={
                'model': Config.OPENAI_MODEL,
                'max_parallel': Config.MAX_PARALLEL_AGENTS
            }
        )
        
        # Lancer l'analyse
        analysis_result = await orchestrator.analyze_ao(documents)
        
        if analysis_result['status'] == 'error':
            logger.error(f"‚ùå Erreur lors de l'analyse: {analysis_result['error']}")
            return analysis_result
        
        # Phase 4: G√©n√©ration des rapports
        logger.info("\nüìä PHASE 4: G√©n√©ration des rapports...")
        report_generator = AOReportGenerator()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Export JSON
        if "json" in Config.EXPORT_FORMATS:
            json_path = Config.REPORTS_PATH / f"ao_analysis_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result['report'], f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"  ‚úì Rapport JSON: {json_path}")
        
        # Export Excel (si implement√©)
        if "excel" in Config.EXPORT_FORMATS:
            try:
                excel_path = Config.REPORTS_PATH / f"ao_analysis_{timestamp}.xlsx"
                report_generator.export_to_excel(analysis_result['report'], excel_path)
                logger.info(f"  ‚úì Rapport Excel: {excel_path}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Export Excel non disponible: {e}")
        
        # Affichage du r√©sum√©
        logger.info("\n" + "="*80)
        logger.info("üìã R√âSUM√â DE L'ANALYSE")
        logger.info("="*80)
        
        summary = analysis_result['report']['executive_summary']
        logger.info(f"Client: {summary['client']}")
        logger.info(f"R√©f√©rence AO: {summary['reference']}")
        logger.info(f"Date limite: {summary['date_limite']}")
        logger.info(f"Volumes estim√©s: {summary['volume_estime']}")
        logger.info(f"Dur√©e contrat: {summary.get('duree_contrat', 'N/A')} mois")
        logger.info(f"Complexit√©: {summary['complexite']}")
        logger.info(f"Risque global: {summary['risque_global']}")
        logger.info(f"Compl√©tude donn√©es: {summary['completude_donnees']}")
        
        go_no_go = summary['go_no_go_recommendation']
        logger.info(f"\nüéØ RECOMMANDATION: {go_no_go['decision']} (Confiance: {go_no_go['confidence']})")
        
        # Actions urgentes
        if analysis_result['report'].get('actions_urgentes'):
            logger.info("\n‚ö° ACTIONS URGENTES:")
            for action in analysis_result['report']['actions_urgentes'][:3]:
                logger.info(f"  ‚Ä¢ {action['action']} ({action['deadline']})")
        
        # Donn√©es manquantes critiques
        if analysis_result['report'].get('donnees_manquantes'):
            logger.info("\n‚ö†Ô∏è DONN√âES MANQUANTES CRITIQUES:")
            for missing in analysis_result['report']['donnees_manquantes'][:5]:
                logger.info(f"  ‚Ä¢ {missing}")
        
        # Statistiques d'ex√©cution
        logger.info("\nüìä STATISTIQUES D'EX√âCUTION:")
        metadata = analysis_result.get('metadata', {})
        if metadata.get('processing_time'):
            logger.info(f"  ‚Ä¢ Temps de traitement: {metadata['processing_time']:.2f} secondes")
        logger.info(f"  ‚Ä¢ Documents trait√©s: {metadata.get('documents_processed', 0)}")
        logger.info(f"  ‚Ä¢ Chunks analys√©s: {metadata.get('chunks_analyzed', 0)}")
        logger.info(f"  ‚Ä¢ Agents ex√©cut√©s: {metadata.get('agents_run', 0)}")
        logger.info(f"  ‚Ä¢ Confiance globale: {analysis_result.get('confidence_global', 0):.1%}")
        
        logger.info("\n‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS!")
        logger.info("="*80)
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}", exc_info=True)
        return {
            'status': 'error',
            'error': str(e),
            'report': {}
        }

def analyze_specific_folder(folder_path: str):
    """
    Fonction helper pour analyser un dossier sp√©cifique
    
    Args:
        folder_path: Chemin vers le dossier √† analyser
    """
    path = Path(folder_path)
    if not path.exists():
        logger.error(f"Le dossier n'existe pas: {path}")
        return
    
    # Lancer l'analyse asynchrone
    result = asyncio.run(analyze_tender_documents(path))
    return result

# ============================================================================
# POINT D'ENTR√âE PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    """
    Point d'entr√©e principal du programme
    Vous pouvez modifier le chemin directement dans Config.DOCUMENTS_PATH
    ou passer un argument en ligne de commande
    """
    
    # Valider la configuration
    try:
        Config.validate()
    except Exception as e:
        logger.error(f"Erreur de configuration: {e}")
        sys.exit(1)
    
    # V√©rifier les arguments de ligne de commande
    if len(sys.argv) > 1:
        # Si un chemin est pass√© en argument, l'utiliser
        custom_path = Path(sys.argv[1])
        if custom_path.exists():
            logger.info(f"Utilisation du chemin personnalis√©: {custom_path}")
            result = asyncio.run(analyze_tender_documents(custom_path))
        else:
            logger.error(f"Le chemin sp√©cifi√© n'existe pas: {custom_path}")
            sys.exit(1)
    else:
        # Utiliser le chemin par d√©faut de Config
        logger.info(f"Utilisation du chemin par d√©faut: {Config.DOCUMENTS_PATH}")
        result = asyncio.run(analyze_tender_documents())
    
    # Afficher le chemin du rapport g√©n√©r√©
    if result['status'] == 'success':
        logger.info(f"\nüìÅ Les rapports ont √©t√© sauvegard√©s dans: {Config.REPORTS_PATH}")
        logger.info("Consultez les fichiers g√©n√©r√©s pour l'analyse d√©taill√©e.")
    else:
        logger.error("L'analyse a √©chou√©. Consultez les logs pour plus de d√©tails.")
        sys.exit(1)